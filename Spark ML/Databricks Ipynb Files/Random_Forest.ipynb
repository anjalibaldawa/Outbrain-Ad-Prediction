{"cells":[{"cell_type":"markdown","source":["## PROJECT GOAL:\nOutbrain is a web advertising platform that displays advertisement boxes of links to pages within websites. It displays links to the sites' pages in addition to sponsored content, generating revenue from the latter.\nIn our project we are predicting whether the particular ad_id will be clicked or not.\n\nThis python files predict ad using random forest classifier algorithm and measure Accuracy and AUC."],"metadata":{}},{"cell_type":"markdown","source":["## Creating a Classification Model using Random Forest Algorithm\n\nIn this file, we will implement a classification model using *Random Forest* that uses features of a Event,Document and Train dataframe to **predict the whether the ad is clicked or not** \n\nYou should follow the steps below to build, train and test the model from the source data:\n\n1. Build a schema of a source data for its Data Frame\n2. Load the Source Data to the schema\n3. Prepare the data with the features (input columns, output column as label)\n4. Split the data using data.randomSplit(): Training and Testing\n5. Transform the columns to a vector using VectorAssembler\n6. set features and label from the vector\n7. Build a ***Random Forest Algorithm*** Model with the label and features\n8. Train the model\n9. Prepare the testing Data Frame with features and label from the vector; Rename label to trueLabel\n10. Predict and test the testing Data Frame using the model trained at the step 8\n11. Compare the predicted result and trueLabel\n\n\n### Import Spark SQL and Spark ML Libraries\n\nFirst, import the libraries you will need:"],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\nfrom pyspark.mllib.tree import RandomForest\nfrom pyspark.ml import Pipeline\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["#####The below command will start spark session when we run our file in oracle BDE. In Databricks keep this cell as False by default. But when you run file in Oracle BDE make it True."],"metadata":{}},{"cell_type":"code","source":["IS_SPARK_SUBMIT_CLI = False\nif IS_SPARK_SUBMIT_CLI:\n    sc = SparkContext.getOrCreate()\n    spark = SparkSession(sc)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Load Source Data\nThe data for this project is provided as a CSV file containing details of advertisement. The data includes specific characteristics (or *features*) for each ad, as well as a *label* column indicating whether the ad was clicked or not.\n\nYou will load this data into a DataFrame and display it."],"metadata":{}},{"cell_type":"code","source":["eventSchema = StructType([\n  StructField(\"display_id\", IntegerType(), False),\n  StructField(\"uuid\", StringType(), False),\n  StructField(\"document_id\", IntegerType(), False),\n  StructField(\"timestamp\", IntegerType(), False),\n  StructField(\"platform\", IntegerType(), False),\n  StructField(\"geo_location\", StringType(), False)\n])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Prepare the Data\nMost modeling begins with exhaustive exploration and preparation of the data. In this example, you will simply select a subset of columns to use as *features* as well as the **ArrDelay** column, which will be the *label* your model will predict."],"metadata":{}},{"cell_type":"markdown","source":["#####Reading all necessary csv file"],"metadata":{}},{"cell_type":"code","source":["if IS_SPARK_SUBMIT_CLI:\n    event = spark.read.csv('eventd.csv', inferSchema=True, header=True)\nelse:\n    event = spark.sql(\"SELECT * FROM eventd_csv\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#create train schema\ntrainSchema = StructType([\n  StructField(\"display_id\", DoubleType(), False),\n  StructField(\"DayOfWeek\", DoubleType(), False),\n  StructField(\"clicked\", IntegerType(), False)\n])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Read traind.csv file\nif IS_SPARK_SUBMIT_CLI:\n    train = spark.read.csv('traind.csv', inferSchema=True, header=True)\nelse:\n    train = spark.sql(\"SELECT * FROM traind_csv\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# create Promoted_Content Schema\npromoted_contentSchema = StructType([\n  StructField(\"ad_id\", IntegerType(), False),\n  StructField(\"document_id\", IntegerType(), False),\n  StructField(\"campaign\", IntegerType(), False),\n  StructField(\"advertiser_id\", IntegerType(), False)\n])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Read Promoted_content.csv file\nif IS_SPARK_SUBMIT_CLI:\n    promoted_content = spark.read.csv('promoted_content.csv', inferSchema=True, header=True)\nelse:\n    promoted_content = spark.sql(\"SELECT * FROM promoted_content_csv\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Below command will join event and train file using disply_id as primary key. It will also drop display_id from event table and ad_id from train table as it create ambiguity error. Then it will assign result to new dataframe data1."],"metadata":{}},{"cell_type":"code","source":["data1=event.join(train,event.display_id==train.display_id).drop(event.display_id).drop(train.ad_id)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Following cell will join two dataframe data1 (which created from joining of event and train table) with Promoted content dataframe using document_id as primary key. Then it will assign result to new dataframe data2."],"metadata":{}},{"cell_type":"code","source":["data2=data1.join(promoted_content,data1.document_id==promoted_content.document_id).drop(data1.document_id)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Following table select disply_id,document_id,platform,ad_id,camaign_id,advertiser_id which are features which help us to train our dataset and clicked column which is label column. Selected column will assign to new dataframe called data."],"metadata":{}},{"cell_type":"code","source":["data =  data2.select(col(\"display_id\").cast(DoubleType()), col(\"document_id\").cast(DoubleType()), col(\"platform\").cast(DoubleType()), col(\"ad_id\").cast(DoubleType()), col(\"campaign_id\").cast(DoubleType()),col(\"advertiser_id\").cast(DoubleType()),col(\"clicked\").alias(\"label\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Split the Data\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."],"metadata":{}},{"cell_type":"code","source":["splits = data.randomSplit([0.7, 0.3])     \ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Prepare the Training Data\nTo train the classification model, you need a training data set that includes a vector of numeric features, and a label column. In this exercise, you will use the **VectorAssembler** class to transform the feature columns into a vector, and then rename the **ArrDelay** column to **label**."],"metadata":{}},{"cell_type":"code","source":["# Import RandomClassifier Algorithm \nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Convert following feature column in one vector for train data\nassembler = VectorAssembler(inputCols = [\"display_id\", \"document_id\", \"platform\", \"ad_id\",\"campaign_id\",\"advertiser_id\"],outputCol=\"normfeatures\")\n#assembler = VectorAssembler(inputCols = [\"clicked\"],outputCol=\"label\")\n\n#Normlize feature data \nminMax = MinMaxScaler(inputCol = assembler.getOutputCol(), outputCol=\"nfeatures\")\n\n#Convert Normlize feature data to vector\nfeatVect = VectorAssembler(inputCols=[\"nfeatures\"], outputCol=\"features\")\n\n#following Random forest algorithm train the classifiction model\ndt = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",impurity=\"gini\",featureSubsetStrategy=\"auto\",numTrees=10,maxDepth=30,maxBins=128,seed=1234)\n\n# Following command will create pipeline with different stages\npipeline = Pipeline(stages=[assembler,minMax,featVect,dt])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["The pipeline itself is an estimator, and so it has a fit method that you can call to run the pipeline on a specified DataFrame. In this case, you will run the pipeline on the training data to train a model."],"metadata":{}},{"cell_type":"code","source":["piplineModel = pipeline.fit(train)\nprint(\"Pipeline complete!\")\n"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Test the Recommender\nThe model produced by the pipeline is a transformed that will apply to all stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, you will transform the test DataFrame using the pipeline to generate label prediction"],"metadata":{}},{"cell_type":"code","source":["# piplineModel with train data set applies test data set and generate predictions\nprediction = piplineModel.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100, truncate=False)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Following cell measure accuracy of Algorithm using MultiClassificationEvaluater and evaluate using predicted data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluation = MulticlassClassificationEvaluator(\n    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluation.evaluate(prediction)\nprint(accuracy)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Following Code will give total Error in project"],"metadata":{}},{"cell_type":"code","source":["print(\"Test Error = %g\" % (1.0 - accuracy))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### Random Forest Evaluator\nCalculate AUC using Random Forest Evaluator."],"metadata":{}},{"cell_type":"code","source":["rf_evaluator =  MulticlassClassificationEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\")\nrf_auc = rf_evaluator.evaluate(prediction)\nprint(\"AUC for Random Forest is= \", rf_auc)"],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"Random_Forest","notebookId":2726509313004450},"nbformat":4,"nbformat_minor":0}
